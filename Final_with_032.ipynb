{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"train_tweets.txt\"\n",
    "stop = stopwords.words('english')\n",
    "nltk.download('wordnet')\n",
    "# file = \"data/train_tweets.txt\"\n",
    "temp = []\n",
    "with open(file,encoding=\"UTF-8\") as data:\n",
    "    for line in data:\n",
    "        row = []\n",
    "        line = line.replace('\\t',\" \")\n",
    "        elem = line.strip().split(\" \")\n",
    "        row.append(elem[0])\n",
    "        row.append(\" \".join(elem[1:]))\n",
    "        temp.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = pd.DataFrame(temp,columns = [\"User\",\"Tweet\"])\n",
    "temp = []\n",
    "elem = []\n",
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# def lemmatize_text(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tw['Tweet'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "# tw['Tweet'].apply(lambda x: [item for item in x if item not in stop])\n",
    "# tw['Tweet'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test=tw.groupby('User').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_user=test.nlargest(7000, 'Tweet')\n",
    "# max_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "tw[\"Tweet\"] = tw['Tweet'].apply(lambda x: html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tw['Tweet']\n",
    "y=tw['User']\n",
    "X[7009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy with 0.308\n",
    "# word_vectorizer = TfidfVectorizer(stop_words = 'english', ngram_range = (1,5),sublinear_tf=True,min_df = 2,max_features=60000)\n",
    "# char_vectorizer = CountVectorizer(analyzer='char', stop_words='english', ngram_range=(2,5), max_features=60000)\n",
    "#0.32\n",
    "word_vectorizer = TfidfVectorizer(stop_words = 'english', ngram_range = (1,6),sublinear_tf=True,min_df = 2,max_features=60000)\n",
    "char_vectorizer = TfidfVectorizer(analyzer='char', stop_words='english', ngram_range=(2,6), max_features=60000)\n",
    "special_vectorizer = TfidfVectorizer(token_pattern='(?u)(@\\w+)|(#\\w+)|(!+)|(\\?+)|(:\\))|(:D)|(:o)|(:O)|(\\.+)', stop_words='english', max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, hstack\n",
    "word_vectorizer.fit(pd.Series(tw['Tweet']))\n",
    "char_vectorizer.fit(pd.Series(tw['Tweet']))\n",
    "special_vectorizer.fit(pd.Series(tw['Tweet']))\n",
    "X_train_dtm_w = word_vectorizer.transform(X_train)\n",
    "X_train_dtm_c = char_vectorizer.transform(X_train)\n",
    "X_train_dtm_special=special_vectorizer.transform(X_train)\n",
    "X_train_dtm = hstack([X_train_dtm_c, X_train_dtm_w,X_train_dtm_special])\n",
    "\n",
    "\n",
    "\n",
    "x_test_count_c=char_vectorizer.transform(X_test)\n",
    "x_test_special=special_vectorizer.transform(X_test)\n",
    "x_test_count_w=word_vectorizer.transform(X_test)\n",
    "X_test_dtm = hstack([x_test_count_c, x_test_count_w,x_test_special])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "model=LinearSVC(max_iter=10000)\n",
    "model.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "y_pred=model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(vec_w,vec_c,vec_special):\n",
    "    file1 = \"test_tweets_unlabeled.txt\"\n",
    "    with open(file1,encoding=\"UTF-8\") as data:\n",
    "        temp = [line for line in data]    \n",
    "    unlabel = pd.Series(temp)\n",
    "    unlabel_dtm_w = vec_w.transform(unlabel)\n",
    "    unlabel_dtm_c = vec_c.transform(unlabel)\n",
    "    unlabel_dtm_special=vec_special.transform(unlabel)\n",
    "    unlabel_dtm= hstack([unlabel_dtm_c,unlabel_dtm_w,unlabel_dtm_special])\n",
    "    return unlabel_dtm\n",
    "    \n",
    "def submission_file(data):\n",
    "    import csv\n",
    "    with open('predicted.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile,lineterminator='\\n')\n",
    "        writer.writerow(['Id','Predicted'])\n",
    "        for count,predicted in enumerate(data):\n",
    "            writer.writerow([count+1,predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_dtm = prepare_test_data(word_vectorizer,char_vectorizer,special_vectorizer) \n",
    "unlabel_pred = model.predict(unlabel_dtm)\n",
    "submission_file(unlabel_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
