{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from utils1 import *\n",
    "import string\n",
    "from pattern.en import suggest\n",
    "import snowballstemmer\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/train_tweets.txt\"\n",
    "temp = []\n",
    "with open(file, 'r') as data:\n",
    "    for line in data:\n",
    "        row = []\n",
    "        line = line.replace('\\t',\" \")\n",
    "        elem = line.strip().split(\" \")\n",
    "        row.append(elem[0])\n",
    "        row.append(\" \".join(elem[1:]))\n",
    "        temp.append(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and data manipulation\n",
    "\n",
    "    Remove and play with features depending on requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def check(x):\n",
    "    try:\n",
    "        x = Counter(list(zip(*x))[1])\n",
    "    except Exception:\n",
    "        x = Counter({})\n",
    "        \n",
    "    return x\n",
    "    \n",
    "def add_pos(tw):\n",
    "    tok_and_tag = lambda x: pos_tag(word_tokenize(str.lower(x)))\n",
    "    tw['tagged_sent'] = tw['Tweet_clean'].apply(tok_and_tag)\n",
    "    possible_tags = sorted(set(list(zip(*chain(*tw['tagged_sent'])))[1]))\n",
    "    def add_pos_with_zero_counts(counter, keys_to_add):\n",
    "        for k in keys_to_add:\n",
    "            counter[k] = counter.get(k, 0)\n",
    "        return counter\n",
    "    \n",
    "    tw['pos_counts'] = tw['tagged_sent'].apply(lambda x: check(x))\n",
    "    tw['sent_vector'] = tw['tagged_sent'].apply(lambda x:\n",
    "    [count for tag, count in sorted(\n",
    "        add_pos_with_zero_counts(\n",
    "            check(x), \n",
    "                    possible_tags).most_common())])\n",
    "    df2 = pd.DataFrame(tw['sent_vector'].tolist())\n",
    "    df2.columns = possible_tags\n",
    "    tw = tw.assign(**df2)\n",
    "    tw = tw.drop(['tagged_sent','pos_counts','sent_vector'], axis=1)\n",
    "    df2 = 0\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_no_tweets = 1\n",
    "threshold = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def text_process(text):\n",
    "    \n",
    "    text = str.lower(text)\n",
    "    tk = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case = False) \n",
    "    text = tk.tokenize(text)\n",
    "    text = [word for word in text if word not in STOPWORDS]\n",
    "    \n",
    "    text = ' '.join(lemmatize(word) for word in text)\n",
    "#     text = ' '.join(PorterStemmer.stem(word) for word in text)\n",
    "#     text = ' '.join(word for word in text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import html\n",
    "import unidecode\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def ret_sen(x):\n",
    "    \n",
    "    try:\n",
    "        ret = pd.Series(analyzer.polarity_scores(x))\n",
    "    except Exception:\n",
    "        ret = pd.Series({'neg':0.0,'neu': 1.0,'pos': 0.0,'compound': 0.0})\n",
    "    return ret\n",
    "\n",
    "def preprocess(tw):\n",
    "    tw = tw.drop_duplicates()\n",
    "    tw=tw[~tw['Tweet_clean'].str.startswith('RT')]\n",
    "    tw[\"Tweet_clean\"] = tw['Tweet_clean'].apply(lambda x: html.unescape(x))\n",
    "    tw[\"Tweet_clean\"] = tw['Tweet_clean'].apply(lambda x: unidecode.unidecode(x))\n",
    "    tw[\"Tweet_clean\"].replace(\"(\\\\r|)\\\\n$\", '', regex=True,inplace=True)\n",
    "    tw['Tweet_clean'].replace(\"(@[A-Za-z0-9]+)\",\"\",regex=True,inplace=True)\n",
    "    tw[\"Tweet_clean\"].replace(r'http.?://[^\\s]+[\\s]?','', regex=True,inplace=True)\n",
    "    tw = tw.reset_index(drop=True)\n",
    "    \n",
    "    tw[['sen_neg','sen_neu','sen_pos','sen_com']] = tw['Tweet_clean'].apply(lambda x: ret_sen(x))\n",
    "    tw = add_pos(tw)\n",
    "    \n",
    "    \n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tw = pd.DataFrame(temp,columns = [\"User\",\"Tweet\"])\n",
    "tw[\"Tweet_clean\"] = tw['Tweet']\n",
    "tw = preprocess(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tw['Tweet_clean'] = tw['Tweet_clean'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_user = tw['User'].value_counts()\n",
    "cnt_user.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "    Using TF-IDF and without sampling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(tw):\n",
    "    tw['num_of_words'] = tw[\"Tweet_clean\"].str.split().apply(len)\n",
    "    tw.drop(tw[tw.num_of_words<1].index, inplace=True)\n",
    "    tw = tw.drop(tw.columns[-1], axis=1)\n",
    "    tw = tw.reset_index(drop=True)\n",
    "    cnt_user = tw['User'].value_counts()\n",
    "    df = pd.DataFrame(cnt_user)\n",
    "    top_user = df[df['User'] >= min_no_tweets].index.tolist()\n",
    "    top_k = tw[tw.User.isin(top_user)]\n",
    "    data = top_k['User'].value_counts()\n",
    "    Tweet = top_k.groupby('User',group_keys=False).apply(lambda x: x.sample(n = min(threshold,len(x))))\n",
    "    Tweet.sample(10)\n",
    "    tw = Tweet\n",
    "    return tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = sample_data(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = tw[\"User\"].value_counts()\n",
    "print(vis.describe())\n",
    "print(tw.shape)\n",
    "tw.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = tw['Tweet_clean'].str.len()\n",
    "\n",
    "plt.hist(length, bins=20, label=\"tweets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f = 60000\n",
    "word1_v = TfidfVectorizer(sublinear_tf = True, ngram_range = (1,4), min_df = 5, token_pattern= r'(?u)[#]*\\b\\w\\w+\\b', max_features= max_f)\n",
    "char_v = TfidfVectorizer(sublinear_tf = True, ngram_range=(2,4), min_df = 5, token_pattern= r'(?u)[#]*\\b\\w\\w+\\b', max_features=max_f, analyzer='char',)\n",
    "spec_char_v = TfidfVectorizer(sublinear_tf = True, token_pattern= r'(?u)(#\\w+)|(!+)|(\\?+)|(:\\))|(:D)|(:o)|(:O)|(\\.+)|(:\\))', max_features = max_f)\n",
    "word1_v.fit(tw.Tweet_clean)\n",
    "char_v.fit(tw.Tweet_clean)\n",
    "spec_char_v.fit(tw.Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment depending on feature selection\n",
    "\n",
    "def stack_features(data):\n",
    "    w1 = word1_v.transform(data['Tweet_clean'])\n",
    "    c1 = char_v.transform(data['Tweet_clean'])\n",
    "    s1 = spec_char_v.transform(data['Tweet'])\n",
    "    print(w1.shape + c1.shape + s1.shape)\n",
    "    feat_1 = data[data.columns[3:7]].values\n",
    "    feat_1 = preprocessing.normalize(feat_1)\n",
    "    feat_2 = data[data.columns[7:]].values\n",
    "    feat_2 = preprocessing.normalize(feat_2)\n",
    "    tf_idf = hstack([w1,c1,s1])\n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tw\n",
    "y = tw.User\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y,random_state=0,test_size = .25)\n",
    "X_train = stack_features(X_train)\n",
    "print(X_train.shape)\n",
    "X_test = stack_features(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various O vs R classifiers Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "lr = LogisticRegression()\n",
    "# nb = ComplementNB(norm = True)\n",
    "nb = MultinomialNB()\n",
    "svm = LinearSVC(max_iter=10000)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, max_features=5000,n_jobs=-1)\n",
    "\n",
    "def test_model(model,X,y):\n",
    "    if model == 'LR':\n",
    "        fit = lr.fit(X, y)\n",
    "        algorithm = 'Logistic Regression'\n",
    "    if model == 'MNB':\n",
    "        fit = nb.fit(X, y)\n",
    "        algorithm = 'Multinomial Naive Bayes'\n",
    "    if model == 'SVC':\n",
    "        fit = svm.fit(X, y)\n",
    "        algorithm = 'Linear SVC'   \n",
    "    if model == 'RF':\n",
    "        fit = rf.fit(X, y)\n",
    "        algorithm = 'Random Forest'\n",
    "    print(algorithm)\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = test_model('SVC',X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-- One Vs Rest --\")\n",
    "# print(\"Weighted F1: {0}\".format(metrics.f1_score(y_test, preds, average=scoring_average)))\n",
    "# print(\"Precision: {0}\".format(metrics.precision_score(y_test, preds, average=scoring_average)))\n",
    "# print(\"Recall: {0}\".format(metrics.recall_score(y_test, preds, average=scoring_average)))\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data():\n",
    "    file1 = \"data/test_tweets_unlabeled.txt\"\n",
    "    with open(file1, 'r') as data:\n",
    "        temp = [line for line in data]    \n",
    "    unlabel = pd.DataFrame(temp,columns = [\"Tweet\"])\n",
    "    unlabel[\"Tweet_clean\"] = unlabel['Tweet']\n",
    "    unlabel = preprocess(unlabel)\n",
    "    unlabel[\"Tweet_clean\"] = unlabel[\"Tweet_clean\"].apply(text_process)\n",
    "    unlabel = stack_features(unlabel)\n",
    "    return unlabel\n",
    "    \n",
    "def submission_file(data):\n",
    "    import csv\n",
    "    with open('predicted.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(['Id','Predicted'])\n",
    "        for count,predicted in enumerate(data):\n",
    "            writer.writerow([count+1,predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_data = prepare_test_data() \n",
    "unlabel_pred = model.predict(unlabel_data)\n",
    "submission_file(unlabel_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
