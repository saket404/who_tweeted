{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8ZoAxBrQK8g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcObrdzWQK8k"
   },
   "source": [
    "# Text Processing\n",
    "\n",
    "    Read each sentence from training file and remove the tabs and extract id and tweet seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cx6OqKQ9QK8l"
   },
   "outputs": [],
   "source": [
    "file = \"data/train_tweets.txt\"\n",
    "temp = []\n",
    "with open(file, 'r',encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        row = []\n",
    "        line = line.replace('\\t',\" \")\n",
    "        elem = line.strip().split(\" \")\n",
    "        row.append(elem[0])\n",
    "        row.append(\" \".join(elem[1:]))\n",
    "        temp.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6phNjx8ZQK8o"
   },
   "outputs": [],
   "source": [
    "tw = pd.DataFrame(temp,columns = [\"User\",\"Tweet\"])\n",
    "temp = []\n",
    "elem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1567418160519,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "ZGYGnOXUQK8q",
    "outputId": "f086b4bc-319d-40e8-8c38-cbfc04581c72"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Let's try and catch up live next week!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8746</td>\n",
       "      <td>Going to watch Grey's on the big screen - Thur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle My pleasure Patrick....hope you are well!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8746</td>\n",
       "      <td>@handle Hi there! Been traveling a lot and lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8746</td>\n",
       "      <td>RT @handle Looking to Drink Clean &amp; Go Green? ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User                                              Tweet\n",
       "0  8746     @handle Let's try and catch up live next week!\n",
       "1  8746  Going to watch Grey's on the big screen - Thur...\n",
       "2  8746  @handle My pleasure Patrick....hope you are well!\n",
       "3  8746  @handle Hi there! Been traveling a lot and lot...\n",
       "4  8746  RT @handle Looking to Drink Clean & Go Green? ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5042,
     "status": "ok",
     "timestamp": 1567418168033,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "QZPL_6LSQK8u",
    "outputId": "1e4c108e-01e8-418c-d6a0-62838f687dda"
   },
   "outputs": [],
   "source": [
    "#data pre-processing\n",
    "tw['num_of_words'] = tw[\"Tweet\"].str.split().apply(len)\n",
    "tw.drop(tw[tw.num_of_words<4].index, inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "tw[\"Tweet\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "len(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1567419110569,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "tX6YRkXZQK8y",
    "outputId": "c06bcc2a-c8f5-4ae6-c780-fcb1b0a3399c"
   },
   "outputs": [],
   "source": [
    "cnt_user = tw['User'].value_counts()\n",
    "cnt_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b5y5S3BW4IL"
   },
   "outputs": [],
   "source": [
    "filtered_users=[]\n",
    "Users=tw.User.unique().tolist()\n",
    "\n",
    "for a in tw.User.unique():\n",
    "    if cnt_user[a]>=50:\n",
    "      filtered_users.append(a)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vy6H5mb0QK81"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "#1000 random sample rows for each author\n",
    "df_new=pd.DataFrame()\n",
    "twts_train=pd.DataFrame()\n",
    "twts_test=pd.DataFrame()\n",
    "author_train=pd.DataFrame()\n",
    "author_test=pd.DataFrame()\n",
    "\n",
    "for a in filtered_users:\n",
    "    rows = random.sample(list(tw[tw['User']==a].index),50)\n",
    "    df_temp = tw.loc[rows]\n",
    "    df_new=df_new.append(df_temp,ignore_index=True) \n",
    "    '''   \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(df_temp.loc[:,['Tweet']], df_temp.loc[:,['User']], test_size=0.2, random_state=42)\n",
    "    twts_train=twts_train.append(X_train, verify_integrity=False)\n",
    "    twts_test=twts_test.append(X_test, verify_integrity=False)\n",
    "    author_train=author_train.append(Y_train, verify_integrity=False)\n",
    "    author_test=author_test.append(Y_test, verify_integrity=False)\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Zg_zCDMxG3z"
   },
   "outputs": [],
   "source": [
    "df_new=tw\n",
    "X = df_new.Tweet\n",
    "y = df_new.User\n",
    "twts_train, twts_test, author_train, author_test = train_test_split(X,y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1567421594243,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "NoWWSpkJQK84",
    "outputId": "462f555c-c838-4089-cc7a-e577ddb39b80"
   },
   "outputs": [],
   "source": [
    "print (len(twts_train),len(author_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1481,
     "status": "ok",
     "timestamp": 1567421598545,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "aVX6UW9kQK87",
    "outputId": "8b931b6d-bdb4-4dc1-a259-bf87b0ec54f1"
   },
   "outputs": [],
   "source": [
    "print(len(twts_test),len(author_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1490,
     "status": "ok",
     "timestamp": 1567421602100,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "c8BN6cX7QK89",
    "outputId": "30e3b2bf-9bbf-4e82-8319-ba2656457510"
   },
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1949,
     "status": "ok",
     "timestamp": 1567421621216,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "qudaex7BT4C9",
    "outputId": "3ad30937-5779-4ac9-bc78-24586506e00f"
   },
   "outputs": [],
   "source": [
    "cnt_user = df_new['User'].value_counts()\n",
    "cnt_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1567421626106,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "Tgp1c45PQK9A",
    "outputId": "979f41f5-372f-4564-97cf-cb5b2866349a"
   },
   "outputs": [],
   "source": [
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1567421629346,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "daImDw7VQK9D",
    "outputId": "8fb2d590-fcd9-47b1-bae6-d02287ed0163"
   },
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1567421632759,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "HnzleMYvQK9G",
    "outputId": "6168877b-01aa-4681-c49a-d8962d875dc2"
   },
   "outputs": [],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxkxXh94QK9J"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def text_process(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Tokenizes and removes punctuation\n",
    "    3. Stems\n",
    "    4. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizing\n",
    "    tknzr = TweetTokenizer()\n",
    "    text_processed=tknzr.tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # steming\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "    \n",
    "\n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1567421640655,
     "user": {
      "displayName": "Aali Alqarni",
      "photoUrl": "",
      "userId": "04001607062444718805"
     },
     "user_tz": -600
    },
    "id": "Vg2WiCF6p1x4",
    "outputId": "def10a43-ca23-4449-fdce-0492bc9e389f"
   },
   "outputs": [],
   "source": [
    "X_train=twts_train\n",
    "X_test=twts_test\n",
    "y_train=author_train\n",
    "y_test=author_test\n",
    "print(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtQXr_BIu6kU"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "lr = LogisticRegression()\n",
    "nb = MultinomialNB()\n",
    "svm = LinearSVC()\n",
    "\n",
    "def test_model(w_vect,c_vect, model):\n",
    "    w_vect.fit(pd.Series(df_new['Tweet']))\n",
    "    c_vect.fit(pd.Series(df_new['Tweet']))         \n",
    "    X_train_dtm_w = w_vect.transform(X_train)\n",
    "    X_train_dtm_c = c_vect.transform(X_train)\n",
    "    X_train_dtm = hstack([X_train_dtm_c, X_train_dtm_w])\n",
    "    \n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    print(f'Rows: {X_train_dtm.shape[0]}')\n",
    "    X_test_dtm_w = w_vect.transform(X_test)\n",
    "    X_test_dtm_c = c_vect.transform(X_test)\n",
    "    X_test_dtm = hstack([X_test_dtm_c, X_test_dtm_w])\n",
    "\n",
    "    \n",
    "    \n",
    "    if model == 'LR':\n",
    "        lr.fit(X_train_dtm, y_train)\n",
    "        y_pred_class = lr.predict(X_test_dtm)\n",
    "        algorithm = 'Logistic Regression'\n",
    "    if model == 'MNB':\n",
    "        nb.fit(X_train_dtm, y_train)\n",
    "        y_pred_class = nb.predict(X_test_dtm)\n",
    "        algorithm = 'Multinomial Naive Bayes'\n",
    "    if model == 'SVC':\n",
    "        svm.fit(X_train_dtm, y_train)\n",
    "        y_pred_class = svm.predict(X_test_dtm)\n",
    "        algorithm = 'Linear SVC'\n",
    "        \n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "    print(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12wPQftVh-SB"
   },
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(analyzer='word',tokenizer= text_process, ngram_range = (1,2),min_df = 1, max_features = 150000)\n",
    "char_vectorizer = CountVectorizer(analyzer='char', stop_words='english', ngram_range=(2,4),max_features=50000)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjGaVJICmPGU"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_model(word_vectorizer, char_vectorizer, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYW59hGj-Gcu"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(w_vect,c_vect):\n",
    "    file1 = \"data/test_tweets_unlabeled.txt\"\n",
    "    with open(file1, 'r',encoding=\"utf-8\") as data:\n",
    "        temp = [line for line in data]    \n",
    "    unlabel = pd.Series(temp,name=\"value\")\n",
    "    data=unlabel.to_frame()\n",
    "    data['num_of_words'] = data[\"value\"].str.split().apply(len)\n",
    "    tw.drop(tw[tw.num_of_words<4].index, inplace=True)\n",
    "    data[\"value\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\n",
    "    data[\"value\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\n",
    "    data[\"value\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\n",
    "    data[\"value\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "    data[\"value\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\n",
    "    data[\"value\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\n",
    "    #print(unlabel)\n",
    "    unlabel_dtm_w = w_vect.transform(data[\"value\"])\n",
    "    unlabel_dtm_c = c_vect.transform(data[\"value\"])\n",
    "    unlabel_dtm = hstack([unlabel_dtm_c, unlabel_dtm_w])\n",
    "    return unlabel_dtm\n",
    "    \n",
    "def submission_file(data):\n",
    "    import csv\n",
    "    with open('predicted.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(['Id','Predicted'])\n",
    "        for count,predicted in enumerate(data):\n",
    "            writer.writerow([count+1,predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY6LFUUE-_qP"
   },
   "outputs": [],
   "source": [
    "unlabel_dtm = prepare_test_data(word_vectorizer, char_vectorizer) \n",
    "unlabel_pred = svm.predict(unlabel_dtm)\n",
    "submission_file(unlabel_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fe6aFFaf_G5u"
   },
   "outputs": [],
   "source": [
    "model_filename = \"linearsvc_model_cw_ngram.pkl\"\n",
    "joblib.dump(svm, model_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "preprocess_New_sampling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
